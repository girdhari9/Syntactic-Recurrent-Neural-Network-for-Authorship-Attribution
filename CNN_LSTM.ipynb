{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-LSTM.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "I3t914ETStyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "427e88c0-8846-4286-ea7e-c1a13d2d50d8"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wI20cQwmSz6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8e314d3e-e591-4dc2-c709-eb5c684f37f8"
      },
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "stop_words = set(stopwords.words('english')) \n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.layers import concatenate,CuDNNLSTM\n",
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "import random\n",
        "\n",
        "from keras.layers import Dense, LSTM,Dropout, Activation,Bidirectional,Reshape,Permute,Multiply,Flatten,Lambda,Layer,TimeDistributed,CuDNNLSTM"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8u-5fFr5amqy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def SentenceToPOS(sentences):\n",
        "    data = []\n",
        "    for sentence in sentences: \n",
        "        one_sentence_pos = [] \n",
        "        \n",
        "        wordsList = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        wordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "        tagged = nltk.pos_tag(wordsList)\n",
        "        \n",
        "        for val in tagged:\n",
        "            one_sentence_pos.append(val[1])\n",
        "        data.append(one_sentence_pos)\n",
        "    return data\n",
        "\n",
        "def Fianl_X_and_Y(novelPOS, novelLabel, sentenceSize):\n",
        "    i, X, Y = 0, [], []\n",
        "    for novel in novelPOS:\n",
        "        start, end = 0, sentenceSize\n",
        "        while(end <= len(novel)):\n",
        "            X.append(novel[start:end])\n",
        "            Y.append(novelLabel[i])\n",
        "            start = end\n",
        "            end += sentenceSize \n",
        "        i += 1\n",
        "    return X, Y\n",
        "\n",
        "def Get_X_and_Y_In_POS_Form(fullPath, sentenceSize):\n",
        "    filenameList = os.listdir(fullPath)\n",
        "    filenameList.sort()\n",
        "    \n",
        "    novelPOS, novelLabel, fileNumber = [], [], 1\n",
        "    for filename in filenameList:\n",
        "        print(fileNumber,\"/\",len(filenameList),\" Done\")\n",
        "        fileNumber += 1\n",
        "        Path = fullPath + filename\n",
        "        with open(Path, 'r') as f:\n",
        "            data = f.read().replace('\"\\n\"','').replace('\\n',' ').replace('- ','')\n",
        "            sentences = sent_tokenize(data)\n",
        "            POS = SentenceToPOS(sentences)\n",
        "            novelPOS.append(POS)\n",
        "            novelLabel.append(filename[8])\n",
        "    return Fianl_X_and_Y(novelPOS, novelLabel, sentenceSize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5wr7ucjBUtYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "19847e5e-b809-44b1-b149-5b43099c1255"
      },
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Phase 1 : Fetching data\")\n",
        "print(\".....................................\")\n",
        "\n",
        "X, Y = Get_X_and_Y_In_POS_Form('/content/gdrive/My Drive/dataset/',100)\n",
        "\n",
        "print(\"Preprocessing Phase 1 : Finished\\n\\n\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing Phase 1 : Fetching data\n",
            ".....................................\n",
            "1 / 33  Done\n",
            "2 / 33  Done\n",
            "3 / 33  Done\n",
            "4 / 33  Done\n",
            "5 / 33  Done\n",
            "6 / 33  Done\n",
            "7 / 33  Done\n",
            "8 / 33  Done\n",
            "9 / 33  Done\n",
            "10 / 33  Done\n",
            "11 / 33  Done\n",
            "12 / 33  Done\n",
            "13 / 33  Done\n",
            "14 / 33  Done\n",
            "15 / 33  Done\n",
            "16 / 33  Done\n",
            "17 / 33  Done\n",
            "18 / 33  Done\n",
            "19 / 33  Done\n",
            "20 / 33  Done\n",
            "21 / 33  Done\n",
            "22 / 33  Done\n",
            "23 / 33  Done\n",
            "24 / 33  Done\n",
            "25 / 33  Done\n",
            "26 / 33  Done\n",
            "27 / 33  Done\n",
            "28 / 33  Done\n",
            "29 / 33  Done\n",
            "30 / 33  Done\n",
            "31 / 33  Done\n",
            "32 / 33  Done\n",
            "33 / 33  Done\n",
            "Preprocessing Phase 1 : Finished\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y6ZaIBkEqh-4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "792288c9-97fb-4f90-a9e0-23873c9e7b64"
      },
      "cell_type": "code",
      "source": [
        "maxval, minval = 0, 999999\n",
        "for doc in X:\n",
        "  for sentence in doc:\n",
        "    minval = min(minval, len(sentence))\n",
        "    maxval = max(maxval, len(sentence))\n",
        "print(maxval, minval)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "155 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dogCj1qYc2nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Labels = np.unique(Y, return_counts = True)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "piwDLTFRdl06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LabelCount = {}\n",
        "for label in Labels:\n",
        "  LabelCount[label] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Gza3XQhddg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MIN_VAL = min(np.unique(Y,return_counts=True)[1]) + 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96JydSd3ees2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dataBalancing(X, Y, LabelCount, MIN_VAL):\n",
        "  X_data = []\n",
        "  for i in range(len(X)):\n",
        "      if(LabelCount[Y[i]] < MIN_VAL):\n",
        "          X_data.append([X[i],Y[i]])\n",
        "          LabelCount[Y[i]] += 1\n",
        "\n",
        "  random.shuffle(X_data)\n",
        "\n",
        "  new_X = []\n",
        "  new_Y = []\n",
        "  for m ,n in X_data:\n",
        "    new_X.append(m)\n",
        "    new_Y.append(n)\n",
        "\n",
        "  return new_X, new_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDd4A2t4fCJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, Y = dataBalancing(X, Y, LabelCount, MIN_VAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUx8rP-nVUJX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tag_to_index_dictionary(X):\n",
        "    tag = set([])\n",
        "    for doc in X:\n",
        "        for sentence in doc:\n",
        "            for word in sentence:\n",
        "                tag.add(word)\n",
        "    tag2index = {t: i + 1 for i, t in enumerate(list(tag))}\n",
        "    tag2index['-PAD-'] = 0\n",
        "    return tag2index\n",
        "\n",
        "def convert_tag_to_sequence_numbers(X, tag2index):\n",
        "    new_X=[]\n",
        "    for doc in X:\n",
        "        new_S = []\n",
        "        for sentence in doc:\n",
        "            new_W = []\n",
        "            for word in sentence:\n",
        "                new_W.append(tag2index[word])\n",
        "            new_S.append(new_W)\n",
        "        new_X.append(new_S)\n",
        "    return new_X\n",
        "\n",
        "\n",
        "def pad_zeros_to_sequence(X, max_length):\n",
        "    new_X = []\n",
        "    for doc in X:\n",
        "        new_X.append(pad_sequences(doc, maxlen=max_length, padding='post'))\n",
        "    return new_X\n",
        "\n",
        "def Encode_Labels(Y):\n",
        "    le = LabelEncoder()\n",
        "    return le.fit_transform(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zpl-BmOWFsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tag2index = tag_to_index_dictionary(X)\n",
        "DICTIONARY_LENGTH = len(tag2index)\n",
        "MAX_LENGTH = 30 # why 30 only ?\n",
        "SENTENCES_IN_NOVEL = 100\n",
        "\n",
        "X = convert_tag_to_sequence_numbers(X, tag2index)\n",
        "X = pad_zeros_to_sequence(X, MAX_LENGTH)\n",
        "X = np.array(X)\n",
        "TOTAL_ROWS = X.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOft0pgkr32P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# My Code Above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMkI-c9c3pN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0a3f8502-afed-4a14-e2f0-13e05dbc6225"
      },
      "cell_type": "code",
      "source": [
        "A = [[[[1,2,3],[4,5,6],[7,8,9]],['A']],[[[12,2,3],[42,5,6],[72,8,9]],['B']],[[[11,2,3],[41,5,6],[71,8,9]],['C']]]\n",
        "random.shuffle(A)\n",
        "A"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[[1, 2, 3], [4, 5, 6], [7, 8, 9]], ['A']],\n",
              " [[[12, 2, 3], [42, 5, 6], [72, 8, 9]], ['B']],\n",
              " [[[11, 2, 3], [41, 5, 6], [71, 8, 9]], ['C']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "vNJZ0a663p6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_X = []\n",
        "for i in range(SENTENCES_IN_NOVEL):\n",
        "    real_X.append(list())\n",
        "for i in range(SENTENCES_IN_NOVEL):\n",
        "    for j in range(TOTAL_ROWS):\n",
        "        real_X[i].append(X[j][i])\n",
        "\n",
        "X_train = []\n",
        "for i in range(SENTENCES_IN_NOVEL):\n",
        "    X_train.append(np.array(real_X[i]))\n",
        "\n",
        "Y = Encode_Labels(Y) \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_binary = to_categorical(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-OUV1a3_3r62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3099571-1524-40ad-c295-0bfa45ae5ea4"
      },
      "cell_type": "code",
      "source": [
        "np.array(y_binary).shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1803, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "uUDusiUz3t7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.engine.topology import Layer as LL\n",
        "class Attention(LL):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        print(self.W_constraint)\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "        print(eij.shape,self.b.shape)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00HGAK4y3wBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "03118b03-de64-4b52-92f5-2bde6cc3fbf2"
      },
      "cell_type": "code",
      "source": [
        "from keras import  regularizers,constraints,initializers\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
        "\n",
        "model = Sequential() \n",
        "\n",
        "FIRST_LSTM = 256\n",
        "SECOND_LSTM = 256\n",
        "EMBEDD_VECTOR_LENGTH = 128\n",
        "outputs=[]\n",
        "inputs_=[]\n",
        "EMB = Embedding(DICTIONARY_LENGTH, EMBEDD_VECTOR_LENGTH, input_length=30)\n",
        "\n",
        "# BDR1 = SpatialDropout1D(0.2)\n",
        "BDR2 = Bidirectional(GRU(FIRST_LSTM, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))\n",
        "BDR3 = Conv1D(FIRST_LSTM, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")\n",
        "BDR4 = GlobalAveragePooling1D()\n",
        "BDR5 = GlobalMaxPooling1D()\n",
        "\n",
        "\n",
        "# BDR = Bidirectional(CuDNNLSTM(FIRST_LSTM,return_sequences=True),merge_mode='concat')\n",
        "TDB = TimeDistributed(Dense(DICTIONARY_LENGTH+3))\n",
        "for i in range(SENTENCES_IN_NOVEL): # SENTENCES_IN_NOVEL = 100\n",
        "    if(i % 10 == 0):\n",
        "      print(i,\"/\",SENTENCES_IN_NOVEL)\n",
        "    inputlayer = Input(shape=[MAX_LENGTH]) # MAX_LENGTH = 30 (30 words per sentence)\n",
        "    inputs_.append(inputlayer)\n",
        "    layer = EMB(inputlayer) # 50 is embedd vector size for each POS Tag\n",
        "#     layer =  BDR1(layer) # output 100 because of bidirectonal concat\n",
        "    layer =  BDR2(layer)\n",
        "    layer =  BDR3(layer)\n",
        "    avg_pool =  BDR4(layer)\n",
        "    max_pool =  BDR5(layer)\n",
        "    layer = concatenate([avg_pool, max_pool]) \n",
        "    layer =  Lambda( lambda x: K.sum(x, axis=1), input_shape=(30,2*FIRST_LSTM))(layer)\n",
        "    outputs.append(layer)\n",
        "merge_ = concatenate(outputs)\n",
        "print(merge_.shape)\n",
        "merge_ = Reshape((SENTENCES_IN_NOVEL, 2*FIRST_LSTM), input_shape=(SENTENCES_IN_NOVEL*(2*FIRST_LSTM),))(merge_)\n",
        "merge_ = Bidirectional(CuDNNLSTM(SECOND_LSTM,return_sequences=True),merge_mode='concat')(merge_)\n",
        "\n",
        "attention_mul = Attention(100)(merge_)#step_dimen =100\n",
        "output = Dense(14, activation='softmax')(attention_mul)\n",
        "\n",
        "model = Model(inputs=inputs_,outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 / 100\n",
            "10 / 100\n",
            "20 / 100\n",
            "30 / 100\n",
            "40 / 100\n",
            "50 / 100\n",
            "60 / 100\n",
            "70 / 100\n",
            "80 / 100\n",
            "90 / 100\n",
            "(?,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a1d6798fe9f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mmerge_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmerge_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCES_IN_NOVEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFIRST_LSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCES_IN_NOVEL\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mFIRST_LSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mmerge_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSECOND_LSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m             if all([s is not None\n\u001b[1;32m    473\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 474\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# input shape known? then we can compute the output shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             return (input_shape[0],) + self._fix_unknown_dimension(\n\u001b[0;32m--> 398\u001b[0;31m                 input_shape[1:], self.target_shape)\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3Gkv4VO15yAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " np.array([1,2,3,4,5,6]).reshape(2,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PpWzVijZ6N10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRRybYtx5yzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Nadam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_binary, epochs=30, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62c7pfAJQzM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "hist = pd.DataFrame(history.history)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"acc\"])\n",
        "plt.plot(hist[\"val_acc\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEgxZUgeYkDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Nadam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "history2 = model.fit(X_train,y_binary,batch_size=16, epochs=30,validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Rp9Tq4NYoq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"acc\"],label = \"Training Accuracy\")\n",
        "plt.plot(hist[\"val_acc\"],label = \"Validation Accuracy\")\n",
        "plt.title('Epoc Number Vs Accuracy') \n",
        "plt.xlabel('Epoc Number') \n",
        "# naming the y axis \n",
        "plt.ylabel('Accuracy')   \n",
        "# show a legend on the plot \n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0hx4KiwQd_wv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}