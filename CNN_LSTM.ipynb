{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "I3t914ETStyu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wI20cQwmSz6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "stop_words = set(stopwords.words('english')) \n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.layers import concatenate,CuDNNLSTM\n",
        "from keras import initializers\n",
        "from keras import backend as K\n",
        "import random\n",
        "\n",
        "from keras.layers import Dense, LSTM,Dropout, Activation,Bidirectional,Reshape,Permute,Multiply,Flatten,Lambda,Layer,TimeDistributed,CuDNNLSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8u-5fFr5amqy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def SentenceToPOS(sentences):\n",
        "    data = []\n",
        "    for sentence in sentences: \n",
        "        one_sentence_pos = [] \n",
        "        \n",
        "        wordsList = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        wordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "        tagged = nltk.pos_tag(wordsList)\n",
        "        \n",
        "        for val in tagged:\n",
        "            one_sentence_pos.append(val[1])\n",
        "        data.append(one_sentence_pos)\n",
        "    return data\n",
        "\n",
        "def Fianl_X_and_Y(novelPOS, novelLabel, sentenceSize):\n",
        "    i, X, Y = 0, [], []\n",
        "    for novel in novelPOS:\n",
        "        start, end = 0, sentenceSize\n",
        "        while(end <= len(novel)):\n",
        "            X.append(novel[start:end])\n",
        "            Y.append(novelLabel[i])\n",
        "            start = end\n",
        "            end += sentenceSize \n",
        "        i += 1\n",
        "    return X, Y\n",
        "\n",
        "def Get_X_and_Y_In_POS_Form(fullPath, sentenceSize):\n",
        "    filenameList = os.listdir(fullPath)\n",
        "    filenameList.sort()\n",
        "    \n",
        "    novelPOS, novelLabel, fileNumber = [], [], 1\n",
        "    for filename in filenameList:\n",
        "        print(fileNumber,\"/\",len(filenameList),\" Done\")\n",
        "        fileNumber += 1\n",
        "        Path = fullPath + filename\n",
        "        with open(Path, 'r') as f:\n",
        "            data = f.read().replace('\"\\n\"','').replace('\\n',' ').replace('- ','')\n",
        "            sentences = sent_tokenize(data)\n",
        "            POS = SentenceToPOS(sentences)\n",
        "            novelPOS.append(POS)\n",
        "            novelLabel.append(filename[8])\n",
        "    return Fianl_X_and_Y(novelPOS, novelLabel, sentenceSize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5wr7ucjBUtYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Preprocessing Phase 1 : Fetching data\")\n",
        "print(\".....................................\")\n",
        "\n",
        "X, Y = Get_X_and_Y_In_POS_Form('/content/gdrive/My Drive/dataset/',100)\n",
        "\n",
        "print(\"Preprocessing Phase 1 : Finished\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6ZaIBkEqh-4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxval, minval = 0, 999999\n",
        "for doc in X:\n",
        "  for sentence in doc:\n",
        "    minval = min(minval, len(sentence))\n",
        "    maxval = max(maxval, len(sentence))\n",
        "print(maxval, minval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dogCj1qYc2nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Labels = np.unique(Y, return_counts = True)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "piwDLTFRdl06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LabelCount = {}\n",
        "for label in Labels:\n",
        "  LabelCount[label] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Gza3XQhddg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MIN_VAL = min(np.unique(Y,return_counts=True)[1]) + 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96JydSd3ees2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dataBalancing(X, Y, LabelCount, MIN_VAL):\n",
        "  X_data = []\n",
        "  for i in range(len(X)):\n",
        "      if(LabelCount[Y[i]] < MIN_VAL):\n",
        "          X_data.append([X[i],Y[i]])\n",
        "          LabelCount[Y[i]] += 1\n",
        "\n",
        "  random.shuffle(X_data)\n",
        "\n",
        "  new_X = []\n",
        "  new_Y = []\n",
        "  for m ,n in X_data:\n",
        "    new_X.append(m)\n",
        "    new_Y.append(n)\n",
        "\n",
        "  return new_X, new_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WDd4A2t4fCJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, Y = dataBalancing(X, Y, LabelCount, MIN_VAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUx8rP-nVUJX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tag_to_index_dictionary(X):\n",
        "    tag = set([])\n",
        "    for doc in X:\n",
        "        for sentence in doc:\n",
        "            for word in sentence:\n",
        "                tag.add(word)\n",
        "    tag2index = {t: i + 1 for i, t in enumerate(list(tag))}\n",
        "    tag2index['-PAD-'] = 0\n",
        "    return tag2index\n",
        "\n",
        "def convert_tag_to_sequence_numbers(X, tag2index):\n",
        "    new_X=[]\n",
        "    for doc in X:\n",
        "        new_S = []\n",
        "        for sentence in doc:\n",
        "            new_W = []\n",
        "            for word in sentence:\n",
        "                new_W.append(tag2index[word])\n",
        "            new_S.append(new_W)\n",
        "        new_X.append(new_S)\n",
        "    return new_X\n",
        "\n",
        "\n",
        "def pad_zeros_to_sequence(X, max_length):\n",
        "    new_X = []\n",
        "    for doc in X:\n",
        "        new_X.append(pad_sequences(doc, maxlen=max_length, padding='post'))\n",
        "    return new_X\n",
        "\n",
        "def Encode_Labels(Y):\n",
        "    le = LabelEncoder()\n",
        "    return le.fit_transform(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zpl-BmOWFsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tag2index = tag_to_index_dictionary(X)\n",
        "DICTIONARY_LENGTH = len(tag2index)\n",
        "MAX_LENGTH = 30 # why 30 only ?\n",
        "SENTENCES_IN_NOVEL = 100\n",
        "\n",
        "X = convert_tag_to_sequence_numbers(X, tag2index)\n",
        "X = pad_zeros_to_sequence(X, MAX_LENGTH)\n",
        "X = np.array(X)\n",
        "TOTAL_ROWS = X.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOft0pgkr32P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# My Code Above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMkI-c9c3pN0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "A = [[[[1,2,3],[4,5,6],[7,8,9]],['A']],[[[12,2,3],[42,5,6],[72,8,9]],['B']],[[[11,2,3],[41,5,6],[71,8,9]],['C']]]\n",
        "random.shuffle(A)\n",
        "A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNJZ0a663p6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_X = []\n",
        "for i in range(SENTENCES_IN_NOVEL):\n",
        "    real_X.append(list())\n",
        "for i in range(SENTENCES_IN_NOVEL):\n",
        "    for j in range(TOTAL_ROWS):\n",
        "        real_X[i].append(X[j][i])\n",
        "\n",
        "X_train = []\n",
        "for i in range(SENTENCES_IN_NOVEL):\n",
        "    X_train.append(np.array(real_X[i]))\n",
        "\n",
        "Y = Encode_Labels(Y) \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_binary = to_categorical(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-OUV1a3_3r62",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(y_binary).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUDusiUz3t7y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.engine.topology import Layer as LL\n",
        "class Attention(LL):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        print(self.W_constraint)\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "        print(eij.shape,self.b.shape)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00HGAK4y3wBb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import  regularizers,constraints,initializers\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
        "\n",
        "model = Sequential() \n",
        "\n",
        "FIRST_LSTM = 256\n",
        "SECOND_LSTM = 256\n",
        "EMBEDD_VECTOR_LENGTH = 128\n",
        "outputs=[]\n",
        "inputs_=[]\n",
        "EMB = Embedding(DICTIONARY_LENGTH, EMBEDD_VECTOR_LENGTH, input_length=30)\n",
        "\n",
        "BDR = SpatialDropout1D(0.2)\n",
        "BDR = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))\n",
        "BDR = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")\n",
        "BDR = concatenate([GlobalAveragePooling1D(), GlobalMaxPooling1D()])\n",
        "\n",
        "# BDR = Bidirectional(CuDNNLSTM(FIRST_LSTM,return_sequences=True),merge_mode='concat')\n",
        "TDB = TimeDistributed(Dense(DICTIONARY_LENGTH+3))\n",
        "for i in range(SENTENCES_IN_NOVEL): # SENTENCES_IN_NOVEL = 100\n",
        "    if(i % 10 == 0):\n",
        "      print(i,\"/\",SENTENCES_IN_NOVEL)\n",
        "    inputlayer = Input(shape=[MAX_LENGTH]) # MAX_LENGTH = 30 (30 words per sentence)\n",
        "    inputs_.append(inputlayer)\n",
        "    layer = EMB(inputlayer) # 50 is embedd vector size for each POS Tag\n",
        "    layer =  BDR(layer) # output 100 because of bidirectonal concat\n",
        "    layer =  Lambda( lambda x: K.sum(x, axis=1), input_shape=(30,2*FIRST_LSTM))(layer)\n",
        "    outputs.append(layer)\n",
        "    \n",
        "merge_ = concatenate(outputs)\n",
        "merge_ = Reshape((SENTENCES_IN_NOVEL, 2*FIRST_LSTM), input_shape=(SENTENCES_IN_NOVEL*(2*FIRST_LSTM),))(merge_)\n",
        "merge_ = Bidirectional(CuDNNLSTM(SECOND_LSTM,return_sequences=True),merge_mode='concat')(merge_)\n",
        "\n",
        "attention_mul = Attention(100)(merge_)#step_dimen =100\n",
        "output = Dense(14, activation='softmax')(attention_mul)\n",
        "\n",
        "model = Model(inputs=inputs_,outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Gkv4VO15yAK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " np.array([1,2,3,4,5,6]).reshape(2,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PpWzVijZ6N10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRRybYtx5yzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Nadam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_binary, epochs=30, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "62c7pfAJQzM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "hist = pd.DataFrame(history.history)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"acc\"])\n",
        "plt.plot(hist[\"val_acc\"])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEgxZUgeYkDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Nadam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "history2 = model.fit(X_train,y_binary,batch_size=16, epochs=30,validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Rp9Tq4NYoq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.plot(hist[\"acc\"],label = \"Training Accuracy\")\n",
        "plt.plot(hist[\"val_acc\"],label = \"Validation Accuracy\")\n",
        "plt.title('Epoc Number Vs Accuracy') \n",
        "plt.xlabel('Epoc Number') \n",
        "# naming the y axis \n",
        "plt.ylabel('Accuracy')   \n",
        "# show a legend on the plot \n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0hx4KiwQd_wv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}