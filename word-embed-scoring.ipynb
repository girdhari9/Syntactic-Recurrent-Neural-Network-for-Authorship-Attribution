{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel():\n",
    "    PATH = os.getcwd()\n",
    "    filename =  'cnn_model.sav'\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "29a8d31b-451e-4427-afb3-fd40cd01bc60",
    "_uuid": "887320b9e2d8efaaa27c3f7913debaf53c37ed52"
   },
   "outputs": [],
   "source": [
    "def stopwordsCreate():    \n",
    "    nltk.download('stopwords')\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    np.random.seed(0)\n",
    "\n",
    "    MAX_NB_WORDS = 100000\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])\n",
    "    return stop_words, tokenizer, MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordEmbedding():\n",
    "    print('loading word embeddings...')\n",
    "    embeddings_index = {}\n",
    "    f = codecs.open('input/fasttext/wiki.simple.vec', encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19aeb800-f1c8-4206-8bf8-323256ff5c14",
    "_uuid": "c600a3f2135982bf6f9b46cce3a97b1e2e5b912c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tweetAnalysis(tweets, stop_words, tokenizer, embeddings_index, MAX_NB_WORDS):\n",
    "    test_df = pd.read_csv('./input' + '/newTest.csv', sep=',', header=0)\n",
    "    test_df = test_df.fillna('_NA_')\n",
    "    label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    raw_docs_test = test_df['comment_text'].tolist() \n",
    "#     raw_docs_test = [tweets,]\n",
    "    num_classes = len(label_names)\n",
    "\n",
    "    processed_docs_test = []\n",
    "    for doc in tqdm(raw_docs_test):\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        filtered = [word for word in tokens if word not in stop_words]\n",
    "        processed_docs_test.append(\" \".join(filtered))\n",
    "    #end for\n",
    "\n",
    "    print(\"tokenizing input data...\")\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_test)  #leaky\n",
    "    word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "    #pad sequences\n",
    "    word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=168)\n",
    "    \n",
    "    #embedding matrix\n",
    "    print('preparing embedding matrix...')\n",
    "    words_not_found = []\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return word_seq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionLabel(word_seq_test):\n",
    "    y_test = loaded_model.predict(word_seq_test)\n",
    "    labelList = []\n",
    "    for val in y_test:\n",
    "        if (val[0] * 100) <= 2:\n",
    "            labelList.append('Normal')\n",
    "        elif (val[0] * 100) > 2 and (val[0] * 100) <= 8:\n",
    "            labelList.append('Less Harmful')\n",
    "        else:\n",
    "            labelList.append('Harmful')\n",
    "    return labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNModel(tweets):\n",
    "    stop_words, tokenizer, MAX_NB_WORDS = stopwordsCreate()\n",
    "    embeddings_index = loadWordEmbedding()\n",
    "    word_seq_test = tweetAnalysis(tweets, stop_words, tokenizer, embeddings_index, MAX_NB_WORDS)\n",
    "    predictedLabel = predictionLabel(word_seq_test)\n",
    "    return predictedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabel = CNNModel('How\\'s the Jaish? Great Sir, I will plant bomb in 12 places in mumbai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
